{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "    - The PEFT config i have chosen is `LORA` as i feel it is a safe bet and a good place to start. It is also very popular and widely trusted and used. \n",
    "    - It is also supported my chosen model.\n",
    "* Model: \n",
    "    - The model i have chosen is `distilbert/distilbert-base-uncased` with a `SequenceClassification` head, based on the Popular `google-bert/bert-base-uncased`.\n",
    "    - Smaller and faster than bert with comparable accuracy.\n",
    "* Evaluation approach: \n",
    "    - The evaluation method i have chosen is `accuracy` which will measure the closeness of a measured value to a standard or known value\n",
    "* Fine-tuning dataset: \n",
    "    The `Dataset` i have chosen is `rotten_tomatoes` a valid data set to use with 2 features text (input) and label (output).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ced996",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9b4f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType\n",
    "from peft import get_peft_model\n",
    "from peft import AutoPeftModelForSequenceClassification\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78566117",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"rotten_tomatoes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19daed42",
   "metadata": {},
   "source": [
    "### Inspect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ce880fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 8530\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "})\n",
      "TEXT Value(dtype='string', id=None)\n",
      "LABEL ClassLabel(names=['neg', 'pos'], id=None)\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN:\", dataset)\n",
    "print(\"TEXT\", dataset[\"train\"].features[f\"text\"])\n",
    "print(\"LABEL\", dataset[\"train\"].features[f\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c290037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE: {'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n",
      "TEXT: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "LABEL: 1\n"
     ]
    }
   ],
   "source": [
    "example = dataset[\"train\"][0]\n",
    "print(\"EXAMPLE:\",example)\n",
    "print(\"TEXT:\",example[\"text\"])\n",
    "print(\"LABEL:\",example[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f8d74",
   "metadata": {},
   "source": [
    "### Create label2id, id2label and label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd3d3173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'neg', 1: 'pos'}\n",
      "{'neg': 0, 'pos': 1}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Create label2id, id2label and label_count\n",
    "labels = dataset[\"train\"].features[f\"label\"].names\n",
    "id2label={}\n",
    "label2id={}\n",
    "for i, name in enumerate(labels):\n",
    "    id2label[i] = name\n",
    "    \n",
    "for i, name in enumerate(labels):\n",
    "    label2id[name] = i\n",
    "    \n",
    "label_count = len(labels);\n",
    "print(id2label)\n",
    "print(label2id)\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a4ff02",
   "metadata": {},
   "source": [
    "### Load Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dec3b14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"neg\",\n",
      "    \"1\": \"pos\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"neg\": 0,\n",
      "    \"pos\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.32.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# if not tokenizer.pad_token:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=label_count, id2label=id2label, label2id=label2id\n",
    ")\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "print(model.config)\n",
    "print(model)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6052e7e",
   "metadata": {},
   "source": [
    "### Inspect Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fe4b8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "OUT ['[CLS]', 'the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'\", 's', 'new', '\"', 'conan', '\"', 'and', 'that', 'he', \"'\", 's', 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarz', '##ene', '##gger', ',', 'jean', '-', 'cl', '##aud', 'van', 'dam', '##me', 'or', 'steven', 'sega', '##l', '.', '[SEP]']\n",
      "WORD_IDS [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 29, 29, 30, 31, 32, 33, 33, 34, 35, 35, 36, 37, 38, 38, 39, None]\n",
      "([CLS],None), (the,0), (rock,1), (is,2), (destined,3), (to,4), (be,5), (the,6), (21st,7), (century,8), (',9), (s,10), (new,11), (\",12), (conan,13), (\",14), (and,15), (that,16), (he,17), (',18), (s,19), (going,20), (to,21), (make,22), (a,23), (splash,24), (even,25), (greater,26), (than,27), (arnold,28), (schwarz,29), (##ene,29), (##gger,29), (,,30), (jean,31), (-,32), (cl,33), (##aud,33), (van,34), (dam,35), (##me,35), (or,36), (steven,37), (sega,38), (##l,38), (.,39), ([SEP],None), "
     ]
    }
   ],
   "source": [
    "example = dataset[\"train\"][0]\n",
    "tokenized_input = tokenizer(example[\"text\"], truncation=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(\"IN\", example[\"text\"])\n",
    "print(\"OUT\", tokens)\n",
    "# word_ids is used to Map output tokens to their respective input word in the raw examples[\"tokens\"]\n",
    "print(\"WORD_IDS\", tokenized_input.word_ids())\n",
    "for i, (token, word_id) in enumerate(zip(tokens, tokenized_input.word_ids())):\n",
    "    print(f\"({token},{word_id})\", end=\", \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ac4c4",
   "metadata": {},
   "source": [
    "### Tokenize and add labels column (which Model expects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44bbdcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1066/1066 [00:00<00:00, 8956.82 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "CONVERTED_TOKENS: ['[CLS]', 'the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'\", 's', 'new', '\"', 'conan', '\"', 'and', 'that', 'he', \"'\", 's', 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarz', '##ene', '##gger', ',', 'jean', '-', 'cl', '##aud', 'van', 'dam', '##me', 'or', 'steven', 'sega', '##l', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "INPUT_IDS: [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1000, 16608, 1000, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "LABELS: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    tokenized[\"labels\"] = examples[\"label\"]\n",
    "    return  tokenized\n",
    "\n",
    "\n",
    "tokenized_input = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "example = tokenized_input[\"train\"][0]\n",
    "text = example[\"text\"]\n",
    "inputIds = example[\"input_ids\"]\n",
    "labels = example[\"labels\"]\n",
    "print(\"TEXT:\",text)\n",
    "print(\"CONVERTED_TOKENS:\",tokenizer.convert_ids_to_tokens(inputIds))\n",
    "print(\"INPUT_IDS:\",inputIds)\n",
    "print(\"LABELS:\",labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42e91d0",
   "metadata": {},
   "source": [
    "### Prepare Evalutation Logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dccbaced",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0f6bda",
   "metadata": {},
   "source": [
    "### Evaluate model before training;\n",
    "Evaluate a test example before training the model.\n",
    "Grab an example with a low accuracy to test after training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f799dd3",
   "metadata": {},
   "source": [
    "Build trainer and then evaluate method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec2981f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_custom_examples(model):\n",
    "    positive_one_review = {\"label\":\"pos\",\"text\":\"I was deeply moved by this film. The storytelling was exceptional, and the characters felt like old friends. The chemistry between the lead actors was palpable, and the cinematography captured the beauty of the setting. The emotional depth and genuine moments left me smiling long after the credits rolled. A must-watch!\"}\n",
    "    positive_two_review = {\"label\":\"pos\",\"text\":\"A timeless masterpiece! The acting, direction, and storytelling are exceptional. The characters’ depth and the emotional journey make this film unforgettable. Definitely a must-watch.\"}\n",
    "    positive_three_review = {\"label\":\"pos\",\"text\":\"A delightful musical with stunning visuals and captivating performances. The chemistry between the leads is electric, and the soundtrack is enchanting. A feel-good movie that leaves you humming its tunes.\"}\n",
    "\n",
    "    \n",
    "    negative_one_review = {\"label\":\"neg\",\"text\":\"I had high hopes for this movie, but it fell flat. The plot was clichéd, and the dialogue felt forced. The pacing was sluggish, and the acting lacked authenticity. I found myself checking my watch, waiting for something interesting to happen. Unfortunately, it never did. Save your time and skip this one.\"}\n",
    "    negative_two_review = {\"label\":\"neg\",\"text\":\"A mind-numbing mess! The plot is convoluted, the dialogue cringe-worthy, and the action sequences lack coherence. It’s a prime example of style over substance.\"}\n",
    "    negative_three_review = {\"label\":\"neg\",\"text\":\"An animated disaster! Shallow characters, a predictable plot, and forced product placements. It’s a soulless cash grab that fails to entertain or engage.\"}\n",
    "    \n",
    "    reviews = [positive_one_review,positive_two_review,positive_three_review,negative_one_review,negative_two_review,negative_three_review]\n",
    "    \n",
    "    input = tokenizer([review[\"text\"] for review in reviews], truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    input.to(device)\n",
    "    model.to(device)\n",
    "    output = model(**input)\n",
    "    print(output.logits.shape)\n",
    "    predictions = torch.argmax(\n",
    "        output.logits, dim=1\n",
    "    )  \n",
    "    print(predictions.shape)\n",
    "    predicted_labels = [\n",
    "        model.config.id2label[prediction.item()] for prediction in predictions\n",
    "    ]\n",
    "    for i, (review, label) in enumerate(zip(reviews, predicted_labels)):\n",
    "        print(f\"Review {i}: MATCHED:{label == review['label']} TEXT:{review['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"rotten_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_input[\"train\"],\n",
    "    eval_dataset=tokenized_input[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caa6f3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 67/67 [00:04<00:00, 14.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6962642073631287, 'eval_accuracy': 0.4924953095684803, 'eval_runtime': 4.9393, 'eval_samples_per_second': 215.822, 'eval_steps_per_second': 13.565}\n",
      "torch.Size([6, 2])\n",
      "torch.Size([6])\n",
      "Review 0: MATCHED:False TEXT:I was deeply moved by this film. The storytelling was exceptional, and the characters felt like old friends. The chemistry between the lead actors was palpable, and the cinematography captured the beauty of the setting. The emotional depth and genuine moments left me smiling long after the credits rolled. A must-watch!\n",
      "Review 1: MATCHED:False TEXT:A timeless masterpiece! The acting, direction, and storytelling are exceptional. The characters’ depth and the emotional journey make this film unforgettable. Definitely a must-watch.\n",
      "Review 2: MATCHED:False TEXT:A delightful musical with stunning visuals and captivating performances. The chemistry between the leads is electric, and the soundtrack is enchanting. A feel-good movie that leaves you humming its tunes.\n",
      "Review 3: MATCHED:True TEXT:I had high hopes for this movie, but it fell flat. The plot was clichéd, and the dialogue felt forced. The pacing was sluggish, and the acting lacked authenticity. I found myself checking my watch, waiting for something interesting to happen. Unfortunately, it never did. Save your time and skip this one.\n",
      "Review 4: MATCHED:True TEXT:A mind-numbing mess! The plot is convoluted, the dialogue cringe-worthy, and the action sequences lack coherence. It’s a prime example of style over substance.\n",
      "Review 5: MATCHED:True TEXT:An animated disaster! Shallow characters, a predictable plot, and forced product placements. It’s a soulless cash grab that fails to entertain or engage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(trainer.evaluate())\n",
    "test_custom_examples(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b8777",
   "metadata": {},
   "source": [
    "You can see above the model has an accuracy of 0.49 (2dp) which is 49% almost random but looking at my custom samples it shows the model is always outputting a negative score, which matches the dataset ratio between negative and positive reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 813,314 || all params: 67,768,324 || trainable%: 1.2001388731407907\n"
     ]
    }
   ],
   "source": [
    "#[\"lin1\",\"lin2\",\"classifier\"]\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, target_modules=['q_lin', 'k_lin', 'v_lin'], inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc0fda9",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 501/1068 [01:35<01:46,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5477, 'learning_rate': 1.0636704119850187e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 50%|█████     | 534/1068 [01:46<01:42,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43598470091819763, 'eval_accuracy': 0.8020637898686679, 'eval_runtime': 5.1584, 'eval_samples_per_second': 206.652, 'eval_steps_per_second': 12.988, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 1000/1068 [03:16<00:13,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4248, 'learning_rate': 1.2734082397003748e-06, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 1068/1068 [03:35<00:00,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42547306418418884, 'eval_accuracy': 0.799249530956848, 'eval_runtime': 5.1559, 'eval_samples_per_second': 206.752, 'eval_steps_per_second': 12.995, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1068/1068 [03:35<00:00,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 215.7604, 'train_samples_per_second': 79.069, 'train_steps_per_second': 4.95, 'train_loss': 0.48288458742005996, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1068, training_loss=0.48288458742005996, metrics={'train_runtime': 215.7604, 'train_samples_per_second': 79.069, 'train_steps_per_second': 4.95, 'train_loss': 0.48288458742005996, 'epoch': 2.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ea9c1",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "936be276",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained (\"rotten_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39ba1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config.to_json_file(\"rotten_model/adapter_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf71a3",
   "metadata": {},
   "source": [
    "Here i use the index for the test example with the lowest accuracy found before training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(\"rotten_model\", num_labels=label_count, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_input[\"train\"],\n",
    "    eval_dataset=tokenized_input[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9596c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c99d94aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:05<00:00, 12.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42547306418418884, 'eval_accuracy': 0.799249530956848, 'eval_runtime': 5.285, 'eval_samples_per_second': 201.704, 'eval_steps_per_second': 12.677}\n",
      "torch.Size([6, 2])\n",
      "torch.Size([6])\n",
      "Review 0: MATCHED:True TEXT:I was deeply moved by this film. The storytelling was exceptional, and the characters felt like old friends. The chemistry between the lead actors was palpable, and the cinematography captured the beauty of the setting. The emotional depth and genuine moments left me smiling long after the credits rolled. A must-watch!\n",
      "Review 1: MATCHED:True TEXT:A timeless masterpiece! The acting, direction, and storytelling are exceptional. The characters’ depth and the emotional journey make this film unforgettable. Definitely a must-watch.\n",
      "Review 2: MATCHED:True TEXT:A delightful musical with stunning visuals and captivating performances. The chemistry between the leads is electric, and the soundtrack is enchanting. A feel-good movie that leaves you humming its tunes.\n",
      "Review 3: MATCHED:True TEXT:I had high hopes for this movie, but it fell flat. The plot was clichéd, and the dialogue felt forced. The pacing was sluggish, and the acting lacked authenticity. I found myself checking my watch, waiting for something interesting to happen. Unfortunately, it never did. Save your time and skip this one.\n",
      "Review 4: MATCHED:True TEXT:A mind-numbing mess! The plot is convoluted, the dialogue cringe-worthy, and the action sequences lack coherence. It’s a prime example of style over substance.\n",
      "Review 5: MATCHED:True TEXT:An animated disaster! Shallow characters, a predictable plot, and forced product placements. It’s a soulless cash grab that fails to entertain or engage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(trainer.evaluate())\n",
    "test_custom_examples(lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c9105",
   "metadata": {},
   "source": [
    "Now the model has an accuracy of 0.80 (2dp) 80% and is correctly predicting the labels for my custom samples. This improvement for a model only over 2 epochs shows how good the PEFT technique is. It has only 1.2% of the original weights trained upon, which is incredible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9895f712",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
