{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "    The PEFT config i have chosen is `LORA` as i feel it is a safe bet and a good place to start. It is also very popular and widely trusted and used. it is also supported my chosen model.\n",
    "* Model: \n",
    "    The model i have chosen is `distilbert/distilbert-base-uncased` with a `TokenClassification` head, based on the Popular `google-bert/bert-base-uncased` but smaller and faster than bert but with comparable accuracy.\n",
    "* Evaluation approach: \n",
    "    The evaluation method i have chosen is `seqeval` which is used for for sequence labeling evaluation and will help me evaluate my chosen data set consisting of ner tags. Uses industry standards like the `GLUE` benchmark\n",
    "* Fine-tuning dataset: \n",
    "    The `Dataset` i have chosen is `WNUT 17: Emerging and Rare entity recognition` a valid data set to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ced996",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9b4f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType\n",
    "from peft import get_peft_model\n",
    "from peft import AutoPeftModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78566117",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 699k/699k [00:00<00:00, 2.75MB/s]\n",
      "Downloading data: 100%|██████████| 90.0k/90.0k [00:00<00:00, 805kB/s]\n",
      "Downloading data: 100%|██████████| 92.2k/92.2k [00:00<00:00, 852kB/s]\n",
      "Generating train split: 100%|██████████| 8530/8530 [00:00<00:00, 1326858.52 examples/s]\n",
      "Generating validation split: 100%|██████████| 1066/1066 [00:00<00:00, 513450.63 examples/s]\n",
      "Generating test split: 100%|██████████| 1066/1066 [00:00<00:00, 494648.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19daed42",
   "metadata": {},
   "source": [
    "### Inspect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ce880fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 8530\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "})\n",
      "TEXT Value(dtype='string', id=None)\n",
      "LABEL ClassLabel(names=['neg', 'pos'], id=None)\n"
     ]
    }
   ],
   "source": [
    "# Look at dataset\n",
    "print(\"TRAIN:\", dataset)\n",
    "print(\"TEXT\", dataset[\"train\"].features[f\"text\"])\n",
    "print(\"LABEL\", dataset[\"train\"].features[f\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c290037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE: {'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n",
      "TEXT: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "LABEL: 1\n"
     ]
    }
   ],
   "source": [
    "# Look at an example\n",
    "example = dataset[\"train\"][0]\n",
    "print(\"EXAMPLE:\",example)\n",
    "print(\"TEXT:\",example[\"text\"])\n",
    "print(\"LABEL:\",example[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f8d74",
   "metadata": {},
   "source": [
    "### Create label2id, id2label and label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd3d3173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'neg', 1: 'pos'}\n",
      "{'neg': 0, 'pos': 1}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Create label2id, id2label and label_count\n",
    "labels = dataset[\"train\"].features[f\"label\"].names\n",
    "id2label={}\n",
    "label2id={}\n",
    "for i, name in enumerate(labels):\n",
    "    id2label[i] = name\n",
    "    \n",
    "for i, name in enumerate(labels):\n",
    "    label2id[name] = i\n",
    "    \n",
    "label_count = len(labels);\n",
    "print(id2label)\n",
    "print(label2id)\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a4ff02",
   "metadata": {},
   "source": [
    "### Load Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dec3b14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert/distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"neg\",\n",
      "    \"1\": \"pos\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"neg\": 0,\n",
      "    \"pos\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.32.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=label_count, id2label=id2label, label2id=label2id\n",
    ")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "print(model.config)\n",
    "print(model)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6052e7e",
   "metadata": {},
   "source": [
    "### Inspect Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fe4b8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "OUT ['[CLS]', 'the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'\", 's', 'new', '\"', 'conan', '\"', 'and', 'that', 'he', \"'\", 's', 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarz', '##ene', '##gger', ',', 'jean', '-', 'cl', '##aud', 'van', 'dam', '##me', 'or', 'steven', 'sega', '##l', '.', '[SEP]']\n",
      "WORD_IDS [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 29, 29, 30, 31, 32, 33, 33, 34, 35, 35, 36, 37, 38, 38, 39, None]\n",
      "len(tokens) 47\n",
      "len(word_ids) 47\n",
      "([CLS],None), (the,0), (rock,1), (is,2), (destined,3), (to,4), (be,5), (the,6), (21st,7), (century,8), (',9), (s,10), (new,11), (\",12), (conan,13), (\",14), (and,15), (that,16), (he,17), (',18), (s,19), (going,20), (to,21), (make,22), (a,23), (splash,24), (even,25), (greater,26), (than,27), (arnold,28), (schwarz,29), (##ene,29), (##gger,29), (,,30), (jean,31), (-,32), (cl,33), (##aud,33), (van,34), (dam,35), (##me,35), (or,36), (steven,37), (sega,38), (##l,38), (.,39), ([SEP],None), "
     ]
    }
   ],
   "source": [
    "example = dataset[\"train\"][0]\n",
    "tokenized_input = tokenizer(example[\"text\"], truncation=True, max_length=200)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(\"IN\", example[\"text\"])\n",
    "print(\"OUT\", tokens)\n",
    "# word_ids is used to Map output tokens to their respective input word in the raw examples[\"tokens\"]\n",
    "print(\"WORD_IDS\", tokenized_input.word_ids())\n",
    "print(\"len(tokens)\", len(tokens))\n",
    "print(\"len(word_ids)\", len(tokenized_input.word_ids()))\n",
    "\n",
    "for i, (token, word_id) in enumerate(zip(tokens, tokenized_input.word_ids())):\n",
    "    print(f\"({token},{word_id})\", end=\", \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ac4c4",
   "metadata": {},
   "source": [
    "### Align labels with tokens, taking into account the special tokens added by tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44bbdcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 8530/8530 [00:00<00:00, 64999.38 examples/s]\n",
      "Map: 100%|██████████| 1066/1066 [00:00<00:00, 44691.62 examples/s]\n",
      "Map: 100%|██████████| 1066/1066 [00:00<00:00, 39907.25 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "CONVERTED_TOKENS: ['[CLS]', 'the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'\", 's', 'new', '\"', 'conan', '\"', 'and', 'that', 'he', \"'\", 's', 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarz', '##ene', '##gger', ',', 'jean', '-', 'cl', '##aud', 'van', 'dam', '##me', 'or', 'steven', 'sega', '##l', '.', '[SEP]']\n",
      "INPUT_IDS: [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1000, 16608, 1000, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102]\n",
      "LABELS: 1\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    tokenized[\"labels\"] = examples[\"label\"]\n",
    "    return  tokenized\n",
    "\n",
    "\n",
    "tokenized_input = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "text = tokenized_input[\"train\"][0][\"text\"]\n",
    "inputIds = tokenized_input[\"train\"][0][\"input_ids\"]\n",
    "labels = tokenized_input[\"train\"][0][\"labels\"]\n",
    "print(\"TEXT:\",text)\n",
    "print(\"CONVERTED_TOKENS:\",tokenizer.convert_ids_to_tokens(inputIds))\n",
    "print(\"INPUT_IDS:\",inputIds)\n",
    "print(\"LABELS:\",labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42e91d0",
   "metadata": {},
   "source": [
    "### Prepare Evalutation Logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8d308e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dccbaced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "label_list = dataset[\"train\"].features[f\"label\"].names\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0f6bda",
   "metadata": {},
   "source": [
    "### Evaluate model before training;\n",
    "Evaluate a test example before training the model.\n",
    "Grab an example with a low accuracy to test after training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f799dd3",
   "metadata": {},
   "source": [
    "Build trainer and then evaluate method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "tokenized_input[\"test\"] = tokenized_input[\"test\"].remove_columns([\"text\"])\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_wnut_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_input[\"train\"],\n",
    "    eval_dataset=tokenized_input[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caa6f3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 67/67 [00:00<00:00, 158.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6927236914634705,\n",
       " 'eval_accuracy': 0.5178236397748592,\n",
       " 'eval_runtime': 0.5088,\n",
       " 'eval_samples_per_second': 2094.988,\n",
       " 'eval_steps_per_second': 131.674}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[\"lin1\",\"lin2\",\"classifier\"]\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, target_modules=[\"lin1\"], inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 776,450 || all params: 67,731,460 || trainable%: 1.1463653669948943\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc0fda9",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 511/1068 [00:07<00:07, 69.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5359, 'learning_rate': 1.0636704119850187e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 50%|█████     | 534/1068 [00:07<00:07, 68.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4237791895866394, 'eval_accuracy': 0.8133208255159474, 'eval_runtime': 0.4555, 'eval_samples_per_second': 2340.315, 'eval_steps_per_second': 147.093, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 1009/1068 [00:15<00:00, 70.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4201, 'learning_rate': 1.2734082397003748e-06, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 1068/1068 [00:17<00:00, 68.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.416022390127182, 'eval_accuracy': 0.8208255159474672, 'eval_runtime': 0.4624, 'eval_samples_per_second': 2305.154, 'eval_steps_per_second': 144.883, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1068/1068 [00:18<00:00, 58.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 18.2049, 'train_samples_per_second': 937.11, 'train_steps_per_second': 58.666, 'train_loss': 0.4736343215913808, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1068, training_loss=0.4736343215913808, metrics={'train_runtime': 18.2049, 'train_samples_per_second': 937.11, 'train_steps_per_second': 58.666, 'train_loss': 0.4736343215913808, 'epoch': 2.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_input[\"train\"],\n",
    "#     eval_dataset=tokenized_input[\"test\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "936be276",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"rotten_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf71a3",
   "metadata": {},
   "source": [
    "Here i use the index for the test example with the lowest accuracy found before training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at 'rotten_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\chris\\miniconda3\\envs\\learning-huggingface\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:286\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 286\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\chris\\miniconda3\\envs\\learning-huggingface\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/rotten_model/resolve/main/adapter_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\chris\\miniconda3\\envs\\learning-huggingface\\lib\\site-packages\\peft\\config.py:144\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m     config_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    145\u001b[0m         pretrained_model_name_or_path, CONFIG_NAME, subfolder\u001b[38;5;241m=\u001b[39msubfolder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs\n\u001b[0;32m    146\u001b[0m     )\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\chris\\miniconda3\\envs\\learning-huggingface\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chris\\miniconda3\\envs\\learning-huggingface\\lib\\site-packages\\huggingface_hub\\file_download.py:1368\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[0;32m   1367\u001b[0m     \u001b[38;5;66;03m# Repo not found => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1368\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1370\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\miniconda3\\envs\\learning-huggingface\\lib\\site-packages\\huggingface_hub\\file_download.py:1238\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1238\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\miniconda3\\envs\\learning-huggingface\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chris\\miniconda3\\envs\\learning-huggingface\\lib\\site-packages\\huggingface_hub\\file_download.py:1631\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[0;32m   1630\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1631\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1640\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32mc:\\Users\\chris\\miniconda3\\envs\\learning-huggingface\\lib\\site-packages\\huggingface_hub\\file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 385\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    386\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    387\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    388\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    390\u001b[0m     )\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\miniconda3\\envs\\learning-huggingface\\lib\\site-packages\\huggingface_hub\\file_download.py:409\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    408\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 409\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\chris\\miniconda3\\envs\\learning-huggingface\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:323\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    315\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    316\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-65d480e1-68cd393d0565d2c845648351;e00b214c-e5f6-461e-8d08-a9d7ac983073)\n\nRepository Not Found for url: https://huggingface.co/rotten_model/resolve/main/adapter_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lora_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoPeftModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrotten_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid2label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel2id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel2id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chris\\miniconda3\\envs\\learning-huggingface\\lib\\site-packages\\peft\\auto.py:73\u001b[0m, in \u001b[0;36m_BaseAutoPeftModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     67\u001b[0m ):\n\u001b[0;32m     68\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m    A wrapper around all the preprocessing steps a user needs to perform in order to load a PEFT model. The kwargs\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    are passed along to `PeftConfig` that automatically takes care of filtering the kwargs of the Hub methods and\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    the config object init.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m PeftConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     74\u001b[0m     base_model_path \u001b[38;5;241m=\u001b[39m peft_config\u001b[38;5;241m.\u001b[39mbase_model_name_or_path\n\u001b[0;32m     76\u001b[0m     task_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(peft_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\chris\\miniconda3\\envs\\learning-huggingface\\lib\\site-packages\\peft\\config.py:148\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m         config_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    145\u001b[0m             pretrained_model_name_or_path, CONFIG_NAME, subfolder\u001b[38;5;241m=\u001b[39msubfolder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs\n\u001b[0;32m    146\u001b[0m         )\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m--> 148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    150\u001b[0m loaded_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_json_file(config_file)\n\u001b[0;32m    151\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclass_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloaded_attributes}\n",
      "\u001b[1;31mValueError\u001b[0m: Can't find 'adapter_config.json' at 'rotten_model'"
     ]
    }
   ],
   "source": [
    "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(\"rotten_model\", num_labels=label_count, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 151.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.416022390127182,\n",
       " 'eval_accuracy': 0.8208255159474672,\n",
       " 'eval_runtime': 0.4595,\n",
       " 'eval_samples_per_second': 2319.662,\n",
       " 'eval_steps_per_second': 145.795}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_input[\"train\"],\n",
    "    eval_dataset=tokenized_input[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c99d94aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  2018,  2152,  8069,  2005,  2023,  3185,  1010,  2021,\n",
      "          2009,  3062,  4257,  1012,  1996,  5436,  2001, 18856, 17322,  2094,\n",
      "          1010,  1998,  1996,  7982,  2371,  3140,  1012,  1996, 15732,  2001,\n",
      "         23667, 17701,  2232,  1010,  1998,  1996,  3772, 10858, 21452,  1012,\n",
      "          1045,  2179,  2870,  9361,  2026,  3422,  1010,  3403,  2005,  2242,\n",
      "          5875,  2000,  4148,  1012,  6854,  1010,  2009,  2196,  2106,  1012,\n",
      "          3828,  2115,  2051,  1998, 13558,  2023,  2028,  1012,   102],\n",
      "        [  101,  1045,  2001,  6171,  2333,  2011,  2023,  2143,  1012,  1996,\n",
      "         20957,  2001, 11813,  1010,  1998,  1996,  3494,  2371,  2066,  2214,\n",
      "          2814,  1012,  1996,  6370,  2090,  1996,  2599,  5889,  2001, 14412,\n",
      "          4502,  3468,  1010,  1998,  1996, 16434,  4110,  1996,  5053,  1997,\n",
      "          1996,  4292,  1012,  1996,  6832,  5995,  1998, 10218,  5312,  2187,\n",
      "          2033,  5629,  2146,  2044,  1996,  6495,  4565,  1012,  1037,  2442,\n",
      "          1011,  3422,   999,   102,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}\n",
      "torch.Size([2, 2])\n",
      "LABEL: neg, REVIEW: I was deeply moved by this film. The storytelling was exceptional, and the characters felt like old friends. The chemistry between the lead actors was palpable, and the cinematography captured the beauty of the setting. The emotional depth and genuine moments left me smiling long after the credits rolled. A must-watch!\n",
      "LABEL: pos, REVIEW: I had high hopes for this movie, but it fell flat. The plot was clichéd, and the dialogue felt forced. The pacing was sluggish, and the acting lacked authenticity. I found myself checking my watch, waiting for something interesting to happen. Unfortunately, it never did. Save your time and skip this one.\n"
     ]
    }
   ],
   "source": [
    "positive_review = \"I was deeply moved by this film. The storytelling was exceptional, and the characters felt like old friends. The chemistry between the lead actors was palpable, and the cinematography captured the beauty of the setting. The emotional depth and genuine moments left me smiling long after the credits rolled. A must-watch!\"\n",
    "negative_review = \"I had high hopes for this movie, but it fell flat. The plot was clichéd, and the dialogue felt forced. The pacing was sluggish, and the acting lacked authenticity. I found myself checking my watch, waiting for something interesting to happen. Unfortunately, it never did. Save your time and skip this one.\"\n",
    "\n",
    "input = tokenizer([positive_review,negative_review], truncation=True, padding=True, return_tensors=\"pt\")\n",
    "print(input)\n",
    "\n",
    "input.to(device)\n",
    "lora_model.to(device)\n",
    "output = lora_model(**input)\n",
    "print(output.logits.shape)\n",
    "predictions = torch.argmax(\n",
    "    output.logits, dim=0\n",
    ")  \n",
    "\n",
    "predicted_labels = [\n",
    "    model.config.id2label[prediction.item()] for prediction in predictions\n",
    "]\n",
    "for review, label in zip([positive_review,negative_review], predicted_labels):\n",
    "    print(f\"LABEL: {label}, REVIEW: {review}\")\n",
    "# for i, z in zip(predicted_labels, tokenizer.tokenize(text)):\n",
    "#     print(f\"({z},{i})\", end=\", \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c9105",
   "metadata": {},
   "source": [
    "As you can see we went from an accuracy of 0.0 to 0.96, an 96% increase for the test sample, showing the model has learnt to predict the correct labels.\n",
    "\n",
    "This is greater than expected and i have been over the code and i cant see anything wrong.\n",
    "\n",
    "Im not sure why im getting such good results for the test example ( which the model hasn't seen and ) but not for the golden gate example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
